---
title: "Inference for the Offspring Distribution"
author: "Jackson Van Vooren"
date: "March 1, 2024"
header-includes:
  - \usepackage{amsthm}
output: workflowr::wflow_html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prerequisites

You should be familiar with the basic theory of branching processes, which is
presented in the **Theory of Branching Processes** vignette. Also, knowledge
of Bayesian statistics, including conjugate priors, Metropolis Hastings, and
deriving the posterior distribution, is required.

## Introduction

Generally, when examining a population, we do not have data on the average number
of offspring. Also, for some organisms, we only collect data on two generations
(parents and children), unless we want to potentially wait years.

An intuitive, but crude, way to estimate the mean number of offspring per individual
is $\frac{\text{number of children}}{\text{number of parents}}$. If some generation
has $100$ individuals and the prior one has $50$, then each parent averaged $2$
offspring.

This, however, provides little insight into the underlying offspring distribution.
A Bayesian approach allows us to incorporate prior knowledge, which is especially
useful if the sample size is small. Bayesian analysis also provides a framework for
quantifying uncertainty in parameter estimates. Instead of providing a single point
estimate, Bayesian methods yield a posterior distribution, which reflects a range
of plausible values. Above all, a Bayesian approach to estimating the mean of the
offspring distribution prepares us to further analyze the probability distribution.

## Graphing a Branching Process

We will use the `simulate_bp` function in the last vignette. This time, we also
keep track of the number of offspring each individual has. Again, we assume this
offspring process follows a Poisson($\lambda$) distribution.

```{r}
simulate_bp <- function(num_generations, lambda) {
  total_individuals <- numeric(num_generations)
  total_individuals[1] <- 1
  offspring_count <- list()

  for (i in 2:num_generations) {
    offspring <- 0
    offspring_per_individual <- numeric(total_individuals[i-1])

    for (j in 1:total_individuals[i-1]) {
      offspring_per_individual[j] <- rpois(1, lambda)
      offspring <- offspring + offspring_per_individual[j]
    }
    if (offspring == 0) {
      total_individuals[i] <- 0
      return(list(total_individuals = total_individuals[1:i], offspring_count = offspring_count))
    }
    total_individuals[i] <- offspring
    offspring_count[[i-1]] <- offspring_per_individual
  }
  return(list(total_individuals = total_individuals, offspring_count = offspring_count))
}
```

A somewhat contrived, though reasonable, example is analyzing the growth of
female octopi in a population. Say some generation of octopi females has just
given birth. We can assume that this models all the octopi in a certain geographic
region. Octopi die soon after giving birth, so the mother does not get counted in
the next generation.

We simulate 20 generations with an offspring distribution that is Poisson($2$).

```{r}
num_generations <- 20
lambda <- 2
bp_simulation <- simulate_bp(num_generations, lambda)
```

The total number of individuals at each generation is:
```{r}
tot_ind <- bp_simulation$total_individuals
tot_ind
```

The 5th generation has $40$ individuals, and each of those has a number of offspring
specified by the corresponding index of `X`. For example, octupus 39 has 1
child (we assume females).
```{r}
X <- bp_simulation$offspring_count[[5]]
X
```

Graphically, the population growth looks exponential.
```{r}
plot_bp <- function(bp_data) {
  total_individuals <- bp_data$total_individuals
  num_generations <- length(total_individuals)
  generations <- 1:num_generations
    plot(generations, total_individuals, type = "l",
       xlab = "Generation", ylab = "Total Individuals",
       main = "Individuals in Branching Process")

  for (i in 1:20) {
    points(i, total_individuals[i], col = "blue", pch = 19)
  }
}
plot_bp(bp_simulation)
```

## Beta-Binomial Conjugates for Mean

### Binomial Likelihood

Recall that the beta and binomial distributions are part of the same conjugate
family. Let's assume that our offspring distribution follows a binomial distribution
with parameters $N$ and $p$.

Let $y_1, y_2, \dots, y_n$ be the counts of offspring in $n$ observations of the
offspring process. So, looking at all the offspring of $k\in \mathbb{N}$ parents.

We fix the $N$ to be some count of offspring greater than or equal

Fix the $N$ to be the largest count of the offspring. From our simulation above,
we observe no parents with more than $6$ offspring, so we can set $N=6$. This
choice is not overly important, as the Metropolis Hastings algorithm will output
a difference proportion. If we believe that a parent could have more offspring,
we could set $N > 6$.

Since the offspring distribution is assumed to be binomial, we have
$$\text{likelihood} = P(y_1, \dots, y_n | p) = \prod_{i=1}^{n} \text{Binomial}(y_i \;;\; N, p)$$
Note here that lowercase $n$ is the number of observations of our offspring process,
while $N$ is the maximum number of offspring we fix.

```{r}
likelihood <- function(data, N, p) {
  return(prod(dbinom(data, size = N, prob = p)))
}
```

### Beta Prior

A possibility is to use a prior $p\sim \text{Beta}(\alpha, \beta)$. Here,
$\alpha, \beta$ are parameters set on the distribution. I assume $\alpha = \beta = 1$.
This is a uniform distribution, so it is the non-informative prior. If we knew
more about the species' behavior, we could potentially set a more informative prior.

```{r}
prior <- function(p, alpha, beta){
  return(dbeta(p, alpha, beta))
}
```

### Metropolis Hastings

Since we are performing inference on a single $p$, I use the standard Metropolis-Hastings
algorithm and assume a normal proposal distribution with a standard deviation of
$0.05$. In this Metropolis-Hastings proposal, the proposal distribution is a normal
random deviate centered around the current value of the parameter. This is the
random walk MH algorithm.

We expect the proposal distribution to be symmetric, so in theory we could use
the Metropolis algorithm. However, we can still include the $Q(x_t|y)$ and $Q(y|x_t)$
in the algorithm, even though they are expected to cancel.

```{r}
beta_binom_MCMC <- function(num_iters, alpha, beta, data, N, proposal_sd) {
  current <- rbeta(1, alpha, beta)
    p <- rep(0, num_iters)
    p[1] <- current
    
    for (i in 2:num_iters) {
      current <- p[i - 1]
      proposal <- rnorm(1, current, proposal_sd)
      
      log_target_prop <- log(prior(proposal, alpha, beta)) + log(likelihood(data, N, proposal))
      log_target_curr <- log(prior(current, alpha, beta)) + log(likelihood(data, N, current))
      
      log_q_curr_given_prop <- log(dnorm(current, proposal, proposal_sd))
      log_q_prop_given_curr <- log(dnorm(proposal, current, proposal_sd))
      
      A <- exp(log_target_prop + log_q_curr_given_prop
                                      - log_target_curr - log_q_prop_given_curr)
  
      if (runif(1) < A) {
        p[i] <- proposal
      } else {
        p[i] <- current
      }
    }
  return(list(p = p))
}
```

Let's run the MCMC on generation 5 of our simulated data from above, `X`. We observe
$n=40$ occurrences of the offspring distribution, which are 1, 4, 4, 2, 0, 3, 2, 4,
3, 1, 1, 0, 0, 0, 2, 5, 3, 1, 1, 3, 2, 3, 0, 2, 1, 3, 1, 6, 3, 5, 1, 5, 1, 2, 4,
3, 2, 1, 1, 0.

```{r}
data <- X
N <- 6  # Max number of trials used in binomial distribution
MCMC_p <- beta_binom_MCMC(num_iters=30000, alpha=1, beta=1, data=data, N=N, proposal_sd=0.05)
mean_p <- mean(MCMC_p$p[20000:30000]) # Perform burn-in and find the mean proportion, p
mean_p
```

Because we assume our offspring distribution is Binomial($N,p$), the mean is

```{r}
mean_offspring <- N * mean_p
mean_offspring
```

This is quite close to the actual $\lambda = 2$. The actual mean of this data is
```{r}
mean(X)
```
which is very close to the MCMC Bayesian estimate.

### Credible Interval

A key benefit to a Bayesian approach (compared to calculating the mean of the data)
is that we can construct a credible interval for plausible values of the binomial
parameter $p$.

```{r}
p_quantiles <- quantile(MCMC_p$p, c(0.05, 0.95))
p_quantiles
```

A 90\% credible interval of $p$ from our posterior samples, then, is $(0.309, 0.410)$.
That is, the mean number of offspring could plausibly range between $(1.85, 2.45)$.
The credible interval shows that this is very likely a supercritical branching
process, as $\mu >> 1$. Indeed, we can use knowledge of the posterior distribution
to calculate this probability.

### Calculating Supercritical Probabilities

#### Example 1. $\lambda >> 1$

Another significant upside of the Bayesian approach is that we can calculate the
probability that this process is supercritical (i.e. the branching process does
not go extinct). We know that the posterior follows
$$p \;|\; \text{data} \sim \text{Beta}\left(\alpha + \sum_{i=1}^{n} y_i \,, \;\; \beta + nN - \sum_{i=1}^{n} y_i\right)$$
Also, recall that a branching process is supercritical when $\mu > 1$, so
$$\mathbb{P}[\text{ supercritical } | \text{ data }] = \mathbb{P}[Np > 1 | \text{ data }]
                                                    = \mathbb{P}[p > 1/N | \text{ data }]$$
Since $p\in [0,1]$ we can integrate the posterior density as follows:
$$\int_{\frac{1}{N}}^{1} f(p \; | \; \text{data}) \; dp = \frac{\int_{\frac{1}{N}}^{1}\text{Beta}(1 + S, 1 + nN - S) \; dp \;\cdot\;\text{Beta}(1, 1)}{\int_{0}^{1}\text{Beta}(1 + S, 1 + nN - S) \; dp \;\cdot\;\text{Beta}(1, 1)}$$

```{r}
S <- sum(X)
n <- length(X)
N <- max(X)

integrand <- function(p){ dbeta(p, shape1 = 1 + S, shape2 = 1 + n * N - S) * dbeta(p, 1, 1)}
numerator <- integrate(integrand, lower = 1/N, upper = 1)
denominator <- integrate(integrand, lower = 0, upper = 1)
p_supercritical <- numerator$value / denominator$value
cat("Supercritical probability:", p_supercritical)
```

As expected, since $\mu = 2$, we almost surely can conclude that this branching
process is supercritical, and the population will persist. Let's say we observe
data that might not be as concusive.

#### Example 2. $\lambda \approx 1$.

Here, assume a vector of offspring counts: $(1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 2, 0)$.
We run the Metropolis Hastings algorithm to get an estimate of the potential
mean parameter of the binomial offspring distribution. The expected value
of the number of offspring is $0.97 < 1$.
```{r}
num_iters <- 30000
X <- c(1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 2, 0)
alpha <- 1
beta <- 1
N <- max(data)

MCMC_p <- beta_binom_MCMC(num_iters, alpha, beta, X, N, proposal_sd=0.02)
mean_p <- mean(MCMC_p$p[20000:30000])
expected_offspring <- mean_p * N
cat("Expected offspring:", expected_offspring)
```

Then, using $p \;|\; \text{data} \sim \text{Beta}\left(\alpha + \sum_{i=1}^{n} y_i \,, \;\; \beta + nN - \sum_{i=1}^{n} y_i\right)$,
```{r}
S <- sum(X)
n <- length(X)
N <- max(X)

integrand <- function(p){ dbeta(p, shape1 = 1 + S, shape2 = 1 + n * N - S) * dbeta(p, 1, 1)}
numerator <- integrate(integrand, lower = 1/N, upper = 1)
denominator <- integrate(integrand, lower = 0, upper = 1)
p_supercritical <- numerator$value / denominator$value
cat("Supercritical probability:", p_supercritical)
```

With $\lambda < 1$, the theory tells us that extinction is inevitable. However,
using knowledge of Bayesian posteriors, we find that the probability this population
persists into perpetuity is about $0.345$.

The binomial assumption works well when we only observe one generation's worth
of the offspring distribution. However, if we have more data, we can use a
different model to conduct inference on each $p_k$ of the offspring distribution.

## Dirichlet-Multinomial Conjugates for Offspring

If we observe multiple generations over time, and keep track of the offspring,
we can perform inference on each of the $p_k$'s, where $1\leq k \leq K$, with $K$
being fixed as the assumed maximum number of offspring.

Let's simulate a new branching process, where we observe $10$ generations of offspring.
For the purpose of the simulation, we assume $\lambda = 1.5$ and the offspring distribution
is distributed Poisson($\lambda$).

```{r}
num_generations <- 10
lambda <- 1.5
bp_simulation <- simulate_bp(num_generations, lambda)
bp_simulation
```

### Multinomial Likelihood

We assume that each generation reproduces independently. Assume there is a maximum
of $N$ offspring and that $p = (p_0, p_1, \dots, p_n)$ is an unknown probability
vector. Each $p_i$ is the probability an individual has $i$ offspring.

Each offspring count can be considered a separate observation, $y_i$, with $1 \leq i \leq n$, where
$n$ is the total number of offspring process observations ($n=289$).

Then, the likelihood of the observed data $y_i$ is
$$\text{likelihood} = P(y_1, \dots, y_n \; | \; p) = \text{Multinomial}(X \;;\; p)$$
where $X = (X_1, \dots, X_N)$ is a vector of counts of data with $0, \dots, N = 6$
offspring in this case. With this simulated data,
```{r}
offspring_observations <- unlist(bp_simulation$offspring_count)
counts <- table(offspring_observations)
counts
X <- as.vector(counts)
```

The likelihood function is,
```{r}
likelihood <- function(data, p) {
  return(dmultinom(data, prob = p))
}
```

### Dirichlet Prior

The natural choice for our prior is the Dirichlet distribution, which is the
conjugate prior of the multinomial likelihood. Also, we have $N$ probabilities,
all of which must sum to $1$, which makes
$$p \sim \text{Dirichlet}(\alpha) \quad\text{ with } \alpha = (\alpha_1, \dots, \alpha_N)$$
a reasonable choice.

```{r}
prior <- function(p, alpha) {
  return(gtools::ddirichlet(p, alpha))
}
```

### Metropolis Hastings

This MCMC algorithm is similar to the beta-binomial algorithm, with one key
difference: the proposal distribution is no longer normal (and no longer symmetric).
Because there are $N$ probabilities that sum to $1$, it makes sense to generate
a random vector following the Dirichlet distribution.

Here, we use an independent Metropolis-Hastings algorithm, where the proposal
does not depend on the current parameter value. With $\alpha = (1, \dots, 1)$,
the proposal distribution is a uniform draw across probabilities. With sufficient
iterations, the MCMC will eventually converge and only accept probabilities that
match the data.

We could modify the algorithm to let the proposal distribution have parameter
`alpha * current`, and we see similar results. In any case, though, the proposal
distribution is not symmetric, and thus must be accounted for in our calculation
of the acceptance probability, `A`. Since $Q(x_t|y) \neq Q(y|x_t)$ in general,
we calculate
$$A = \min \left(1, \frac{\pi(y)Q(x_t|y)}{\pi(x_t) Q(y|x_t)}\right) \quad \text{where }\;\; \pi(x) = \text{prior} \cdot \text{likelihood} = \text{target distribution}$$
```{r}
mcmc_multinomial_dirichlet <- function(num_iters, alpha, data) {
  p_list <- list()
  p_list[[1]] <- rep(1/length(alpha), length(alpha))
  
  for (i in 2:num_iters) {

    current <- p_list[[i - 1]]
    proposal <- gtools::rdirichlet(1, alpha)
    
    log_target_prop <- log(prior(proposal, alpha)) + log(likelihood(data, proposal))
    log_target_curr <- log(prior(current, alpha)) + log(likelihood(data, current))

    log_q_curr <- log(gtools::ddirichlet(current, alpha))
    log_q_prop <- log(gtools::ddirichlet(proposal, alpha))
    
    A <- exp(log_target_prop - log_target_curr + log_q_curr - log_q_prop)
    
    if (runif(1) < A) {
      p_list[[i]] <- proposal
    } else {
      p_list[[i]] <- current
    }
  }
  return(p_list)
}
```

Using `X` as the vector of offspring counts,
```{r}
num_iters <- 30000
alpha <- rep(1, length(X)) # Non-informative prior
MCMC_p <- mcmc_multinomial_dirichlet(num_iters, alpha, X)
mean_p <- colMeans(do.call(rbind, MCMC_p[20000:30000]))
mean_p
```

That is, the probability that each individual has $0$ through $N$ offspring is
$$\{p_k\} = (p_0, p_1, p_2, p_3, p_4, p_5, p_6) = (0.204, 0.335, 0.239, 0.157, 0.0427, 0.0135, 0.0077)$$

### Calculating the Extinction Probability

Recall that the extinction probability is the smallest solution to $\phi(s) = s$,
where $\phi$ is the probability generating function. Thus, we solve
$$\phi(s) = 0.204\cdot s^0 + 0.335 \cdot s^1 + 0.239\cdot s^2 + 0.157\cdot s^3 + 0.0427 \cdot s^4 + 0.0135 \cdot s^5 + 0.0077 \cdot s^6 = s$$

```{r}
# Subtract 1 from the s term to find roots of phi(s) = s
polyroot(c(0.204, 0.335-1, 0.239, 0.157, 0.0427, 0.0135, 0.0077))
```

The two real solutions are $0.369$ and $1$. So, $P_e$ of this branching process
is $0.369$.

This Dirichlet method is especially powerful. By only assuming the non-informative
Dirichlet (uniform) prior, we can then get an estimate of the entire offspring
probability vector. From there, we can solve for the roots of a polynomial and
estimate the probability that the population goes extinct.

All simulations for this data followed a Poisson distribution, but both binomial
and multinomial likelihoods can be used to get very close estimtes on the true
offspring probability distribution.

## Closing Remarks

This vignette covered a variety of methods to perform inference on the offspring
distribution, both for its mean (using beta-binomial conjugacy) and each offspring
probability (with dirichlet-multinomial priors/likelihoods).

These branching processes, though, are all of a ``single'' type. That is, they
model population growth solely in numbers of individuals. The next vignette
presents an extension of the single-type branching process, which is known
as the multitype Galton Watson process.

[Cloez](https://hal.science/hal-01994949/document) explains the benefits of a Dirichlet
prior in modeling branching processes. The paper also explains different types of
inference, such as the extinction time and the short-term viability of the population.

A Poisson model is presented in [Angelov](https://www.sciencedirect.com/science/article/pii/S0898122112000703#br000090).
This inspired the idea of using a Binomial likelihood to perform inference on the mean of the Galton Watson process.

