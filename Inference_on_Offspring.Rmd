---
title: "Inference for Offspring Mean"
author: "Jackson Van Vooren"
date: "March 1, 2024"
header-includes:
  - \usepackage{amsthm}
output: workflowr::wflow_html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prerequisites

You should be familiar with the basic theory of branching processes, which is
presented in the **Theory of Branching Processes** vignette. Also, knowledge
of Bayesian statistics, including conjugate priors, Metropolis Hastings, and
deriving the posterior distribution, is required.

## Introduction

Generally, when examining a population, we do not have data on the average number
of offspring. Also, for some organisms, we only collect data on two generations
(parents and children), unless we want to potentially wait years.

An intuitive, but crude, way to estimate the mean number of offspring per individual
is $\frac{\text{number of children}}{\text{number of parents}}$. If some generation
has $100$ individuals and the prior one has $50$, then each parent averaged $2$
offspring.


This, however, provides little insight into the underlying offspring distribution.
A Bayesian approach allows us to incorporate prior knowledge, which is especially
useful if the sample size is small. Bayesian analysis also provides a framework for
quantifying uncertainty in parameter estimates. Instead of providing a single point
estimate, Bayesian methods yield a posterior distribution, which reflects a range
of plausible values. Above all, a Bayesian approach to estimating the mean of the
offspring distribution prepares us to further analyze the probability distribution.

## Graphing a Branching Process

We will use the `simulate_bp` function in the last vignette. This time, we also
keep track of the number of offspring each individual has. Again, we assume this
offspring process follows a Poisson($\lambda$) distribution.

```{r}
simulate_bp <- function(num_generations, lambda) {
  total_individuals <- numeric(num_generations)
  total_individuals[1] <- 1
  offspring_count <- list()

  for (i in 2:num_generations) {
    offspring <- 0
    offspring_per_individual <- numeric(total_individuals[i-1])

    for (j in 1:total_individuals[i-1]) {
      offspring_per_individual[j] <- rpois(1, lambda)
      offspring <- offspring + offspring_per_individual[j]
    }
    if (offspring == 0) {
      total_individuals[i] <- 0
      return(list(total_individuals = total_individuals[1:i], offspring_count = offspring_count))
    }
    total_individuals[i] <- offspring
    offspring_count[[i-1]] <- offspring_per_individual
  }
  return(list(total_individuals = total_individuals, offspring_count = offspring_count))
}
```

A somewhat contrived, though reasonable, example is analyzing the growth of
female octopi in a population. Say some generation of octopi females has just
given birth. We can assume that this models all the octopi in a certain geographic
region. Octopi die soon after giving birth, so the mother does not get counted in
the next generation.

We simulate 20 generations with an offspring distribution that is Poisson($2$).

```{r}
num_generations <- 20
lambda <- 2
bp_simulation <- simulate_bp(num_generations, lambda)
```

The total number of individuals at each generation is:
```{r}
tot_ind <- bp_simulation$total_individuals
tot_ind
```

The 5th generation has $40$ individuals, and each of those has a number of offspring
specified by the corresponding index of `X`. For example, octupus 39 has 1
child (we assume females).
```{r}
X <- bp_simulation$offspring_count[[5]]
X
```

Graphically, the population growth looks exponential.
```{r}
plot_bp <- function(bp_data) {
  total_individuals <- bp_data$total_individuals
  num_generations <- length(total_individuals)
  generations <- 1:num_generations
    plot(generations, total_individuals, type = "l",
       xlab = "Generation", ylab = "Total Individuals",
       main = "Individuals in Branching Process")

  for (i in 1:20) {
    points(i, total_individuals[i], col = "blue", pch = 19)
  }
}
plot_bp(bp_simulation)
```

## Beta-Binomial Conjugates for Mean

### Binomial Likelihood

Recall that the beta and binomial distributions are part of the same conjugate
family. Let's assume that our offspring distribution follows a binomial distribution
with parameters $N$ and $p$.

Let $y_1, y_2, \dots, y_n$ be the counts of offspring in $n$ observations of the
offspring process. So, looking at all the offspring of $k\in \mathbb{N}$ parents.

We fix the $N$ to be some count of offspring greater than or equal

Fix the $N$ to be the largest count of the offspring. From our simulation above,
we observe no parents with more than $6$ offspring, so we can set $N=6$. This
choice is not overly important, as the Metropolis Hastings algorithm will output
a difference proportion. If we believe that a parent could have more offspring,
we could set $N > 6$.

Since the offspring distribution is assumed to be binomial, we have
$$\text{likelihood} = P(y_1, \dots, y_n | p) = \prod_{i=1}^{n} \text{Binomial}(y_i \;;\; N, p)$$
Note here that lowercase $n$ is the number of observations of our offspring process,
while $N$ is the maximum number of offspring we fix.

```{r}
likelihood <- function(data, K, p) {
  return(prod(dbinom(data, size = K, prob = p)))
}
```

### Beta Prior

A possibility is to use a prior $p\sim \text{Beta}(\alpha, \beta)$. Here,
$\alpha, \beta$ are parameters set on the distribution. I assume $\alpha = \beta = 1$.
This is a uniform distribution, so it is the non-informative prior. If we knew
more about the species' behavior, we could potentially set a more informative prior.

```{r}
prior <- function(p, a, b){
  return(dbeta(p, a, b))
}
```

### Metropolis Hastings

Since we are performing inference on a single $p$, I use the standard Metropolis-Hastings
algorithm and assume a normal proposal distribution with a standard deviation of
$0.05$.

```{r}
beta_binom_MCMC <- function(num_iters, alpha, beta, data, N, proposal_sd) {
  current <- rbeta(1, alpha, beta)
  p <- rep(0, num_iters)
  p[1] <- current
  
  for (i in 2:num_iters) {
    current <- p[i - 1]
    proposal <- current + rnorm(1, 0, proposal_sd)
    
    A <- exp(log(prior(proposal, alpha, beta)) + log(likelihood(data, N, proposal))
                 - log(prior(current, alpha, beta)) - log(likelihood(data, N, current)))

    if (runif(1) < A) {
      p[i] <- proposal
    } else {
      p[i] <- current
    }
  }
  return(list(p = p))
}
```

Let's run the MCMC on generation 5 of our simulated data from above, `X`. We observe
$n=40$ occurrences of the offspring distribution, which are 1, 4, 4, 2, 0, 3, 2, 4,
3, 1, 1, 0, 0, 0, 2, 5, 3, 1, 1, 3, 2, 3, 0, 2, 1, 3, 1, 6, 3, 5, 1, 5, 1, 2, 4,
3, 2, 1, 1, 0.

```{r}
data <- X
N <- 6  # Max number of trials used in binomial distribution
result <- beta_binom_MCMC(num_iters=30000, alpha=1, beta=1, data=data, N=N, proposal_sd=0.05)
MCMC_p <- mean(result$p[20000:N]) # Perform burn-in and find the mean proportion, p
MCMC_p
```

Because we assume our offspring distribution is Binomial($N,p$), the mean is

```{r}
mean_offspring <- N * MCMC_p
mean_offspring
```

This is quite close to the actual $\lambda = 2$.

The mean of this data is
```{r}
mean(X)
```
which is very close to the MCMC Bayesian estimate.

### Credible Interval

TODO

### Calculating Supercritical Probability

#### Example 1. $\lambda >> 1$

Another significant upside of the Bayesian approach is that we can calculate the
probability that this process is supercritical (i.e. the branching process does
not go extinct). We know that the posterior follows
$$p \;|\; \text{data} \sim \text{Beta}\left(\alpha + \sum_{i=1}^{n} y_i \,, \;\; \beta + nN - \sum_{i=1}^{n} y_i\right)$$
Also, recall that a branching process is supercritical when $\mu > 1$, so
$$\mathbb{P}[\text{ supercritical } | \text{ data }] = \mathbb{P}[Np > 1 | \text{ data }]
                                                    = \mathbb{P}[p > 1/N | \text{ data }]$$
Since $p\in [0,1]$ we can integrate the posterior density as follows:
$$\int_{\frac{1}{N}}^{1} f(p \; | \; \text{data}) \; dp = \frac{\int_{\frac{1}{N}}^{1}\text{Beta}(1 + S, 1 + nN - S) \; dp \;\cdot\;\text{Beta}(1, 1)}{\int_{0}^{1}\text{Beta}(1 + S, 1 + nN - S) \; dp \;\cdot\;\text{Beta}(1, 1)}$$

```{r}
S <- sum(X)
n <- length(X)
N <- max(X)

integrand <- function(p){ dbeta(p, shape1 = 1 + S, shape2 = 1 + n * N - S) * dbeta(p, 1, 1)}
numerator <- integrate(integrand, lower = 1/N, upper = 1)
denominator <- integrate(integrand, lower = 0, upper = 1)
p_supercritical <- numerator$value / denominator$value
cat("Supercritical probability", p_supercritical)
```


As expected, since $\mu = 2$, we almost surely can conclude that this branching
process is supercritical, and the population will persist. Let's say we observe
data that might not be as concusive.

#### Example 2. $\lambda \approx 1$.

We run the Metropolis Hastings algorithm to get an estimate of the potential
mean parameter of the binomial offspring distribution. The expected value
of the number of offspring is $0.97 < 1$.
```{r}
num_iters <- 30000
X <- c(1,1,1,0,1,1,2,1,0,1,2,0)
alpha <- 1
beta <- 1
N <- max(data)

MCMC_p <- beta_binom_MCMC(num_iters, alpha, beta, X, N, proposal_sd=0.01)
mean_p <- mean(MCMC_p$p[20000:N])
expected_offspring <- mean_p * N
cat("Expected offspring:", expected_offspring)
```

Then, using $p \;|\; \text{data} \sim \text{Beta}\left(\alpha + \sum_{i=1}^{n} y_i \,, \;\; \beta + nN - \sum_{i=1}^{n} y_i\right)$,
```{r}
S <- sum(X)
n <- length(X)
N <- max(X)

integrand <- function(p){ dbeta(p, shape1 = 1 + S, shape2 = 1 + n * N - S) * dbeta(p, 1, 1)}
numerator <- integrate(integrand, lower = 1/N, upper = 1)
denominator <- integrate(integrand, lower = 0, upper = 1)
p_supercritical <- numerator$value / denominator$value
cat("Supercritical probability", p_supercritical)
```

With $\lambda < 1$, the theory tells us that extinction is inevitable. However,
using knowledge of Bayesian posteriors, we find that the probability this population
persists into perpetuity is about $0.345$.

The binomial assumption works well when we only observe one generation's worth
of the offspring distribution. However, if we have more data, we can use a
different model to conduct inference on each $p_k$ of the offspring distribution.

## Dirichlet-Multinomial Conjugates for Offspring

If we observe multiple generations over time, and keep track of the offspring,
we can perform inference on each of the $p_k$'s, where $1\leq k \leq K$, with $K$
being fixed as the assumed maximum number of offspring.

Our simulation code keeps track of the number of offspring each individual has.
The below code takes in these counts and creates a summary matrix of the number
of offspring per individual in each generation.

```{r}
create_offspring_summary <- function(offspring_count) {
  max_offspring <- max(unlist(offspring_count))
  num_generations <- length(offspring_count)
  offspring_summary <- matrix(0, nrow = num_generations, ncol = max_offspring + 1)
  colnames(offspring_summary) <- 0:max_offspring
  
  for (i in 1:length(offspring_count)) {
    counts <- table(offspring_count[[i]])
    offspring_summary[i, 1:length(counts)] <- counts
  }
  return(offspring_summary)
}
```

Let's simulate a new branching process, where we observe $10$ generations of offspring.
For purpose of the simulation, we assume $\lambda = 1.5$ and the offspring distribution
is distributed Poisson($\lambda$).

```{r}
num_generations <- 10
lambda <- 1.5
bp_simulation <- simulate_bp(num_generations, lambda)

offspring_summary <- create_offspring_summary(bp_simulation$offspring_count)
offspring_summary
```
This shows us the counts of number of offspring per individual for each of the generations simulated.

Likelihood.

Prior.

Etc.

```{r}
c <- colSums(offspring_summary, na.rm = TRUE)
X <- as.vector(c)
X
```

```{r}

# PRIOR SHOULDNT HAVE p? TODO. LOOK AT THE EXAMPLE ONLINE I FOUND.
prior <- function(p, alpha) {
  return((log(gtools::ddirichlet(p, alpha))))
}

likelihood <- function(data, p) {
  return(log(dmultinom(data, prob = p)))
}


```

```{r}
mcmc_multinomial_dirichlet <- function(N, alpha, data) {
  p_list <- list()
  p_list[[1]] <- rep(1/length(alpha), length(alpha))
  
  for (i in 1:N) {  # Start loop from i = 1

    current <- p_list[[i]]
    proposal <- gtools::rdirichlet(1, alpha)

    prior1 <- prior(proposal, alpha)
    lik1 <- likelihood(data, proposal)
    prior2 <- prior(current, alpha)
    lik2 <- likelihood(data, current)
    
    A <- exp(prior1 + lik1 - prior2 - lik2)
    
    if (runif(1) < A) {
      p_list[[i + 1]] <- proposal
    } else {
      p_list[[i + 1]] <- current
    }
  }
  
  return(p_list)
}

# Example usage: # for reproducibility
N <- 20000  # Number of iterations
X <- as.vector(c)
alpha <- rep(1, length(X))
result <- mcmc_multinomial_dirichlet(N, alpha, X)
result[[N]]


```

From a uniform distribution, we are able to simulate probabilities that represent the posterior.
Theoretically, we can find the Dirichlet posterior.

Proof of Dirichlet - Multinomial conjugacy?

And our simulated data actually follows a Poisson offspring with mean 2 or 1.5
In both cases, without even knowing Poisson, we can perform inference on the BP!

So, given all this data, we now have Bayesian estimates for each offspring probability.

Let's do a posterior credible interval.

Note that this method still works if we only observe two generations (parent and child).
Of course, more data is helpful in getting higher counts.

Also phi(s) = s.

Make sure to solve this from the data LOL. Use R it should not be that hard.

BASICALLY WE DO HAVE A PROBLEM IF WE CANNOT OBSERVE OFFSPRING (SAY A RAPIDLY REPRODUCING BACTERIA COLONY).
THIS MAKES SENSE THO, TO PERFORM INFERENCE ON WHAT THE OFFSPRING DISTRIBUTION IS, WE NEED TO OBSERVE
THE NUMBER OF OFFSPRING.

SAY WE WERE GIVEN INITIAL AND FINAL POPULATION NUMBERS.
I.E. 1 AND 80? I MEAN WE'D NEED TO KNOW THE NUMBER OF GENERATIONS?
1
2
4
7
10

I DOUBT THIS IS EVEN POSSIBLE TBH? LIKE WE'D HAVE TO LOOK AT ALL POSSIBLE
OFFSPRING DISTRIBUTION AND FIND THE MOST PROBABLE ONE?
Like
first one we have 1 offspring
second one
either 1 1 or 2 0 or 0 2
Then,
2 2 2 1 or 1 3 0 3 or 7 0 0 0 or whatever. You get the gist.
We could assume an equal.

```{r}
library(partitions)

restrictedparts(7, 4)

```


Then, we can do MCMC on each observation of offspring
 4 * (7, 0, 0, 0)
 12 * (6, 1, 0, 0)
 12 * (5, 2, 0, 0)
 12 * (5, 1, 1, 0)
 24 * (4, 2, 1, 0)
 12 * (3, 3, 1, 0)
 12 * (3, 2, 2, 0)
 4 * (4, 1, 1, 1)
 12 * (3, 2, 1, 1)
 4 * (2, 2, 2, 1)
 
 These are all the possible offspring observations (there are 12 ways to get (3, 3, 1, 0), for example).
 
 If we MCMC each one, take the expected probability, multiply it by the corresponding weight, and take the mean.
 
 
 
 I was not able to find papers on this, but it is essentially a Monte Carlo estimate.
 

```{r}

# Example usage:
N <- 30000  # Number of iterations
vectors <- list(
  c(7, 0, 0, 0),
  c(6, 1, 0, 0),
  c(5, 2, 0, 0),
  c(5, 1, 1, 0),
  c(4, 2, 1, 0),
  c(3, 3, 1, 0),
  c(3, 2, 2, 0),
  c(4, 1, 1, 1),
  c(3, 2, 1, 1),
  c(2, 2, 2, 1)
)
a <- 1     # Shape parameter of prior
b <- 1     # Shape parameter of prior
K <- 7  # Number of trials in binomial distribution


# result <- betabin_tour(N, a, b, c(2,2,2,1), K)
# print(mean(result$p[20000:N]))


```

```{r}
4 * 0.1859731 +
 12 * 0.1975058 +
 12 * 0.1925846 +
 12 * 0.1820622 +
 24 * 0.1952222 +
 12 * 0.193863 +
 12 * 0.2004157 +
 4 * 0.1921681 +
 12 * 0.1888514 +
 4 * 0.182556
```





```{r}
20.79151/(4+12+12+12+24+12+12+4+12+4)
```

So mean $p$ is $0.192514$, implying $np \approx 2$.

Could be a viable way if we only have the populations at 2 times, and no observations
on the individual number of offspring.
This for example, would be if we had a colony of bacteria, counted 100, then 250.
Of course, permutations get ugly but still? Cool Monte Carlo estimate.

